```yaml
name: One shot 2024-06 (incremental merge)

on:
  workflow_dispatch:

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install pandas pyarrow duckdb

      - name: Bronze to silver (days) for 202406
        run: |
          python - <<'PY'
          import json, re
          import pandas as pd
          from pathlib import Path

          ROOT = Path(".").resolve()
          BRONZE = (ROOT / "data" / "bronze").resolve()
          SILVER = (ROOT / "data" / "silver").resolve()
          SILVER.mkdir(parents=True, exist_ok=True)

          only_month = "202406"
          date_re = re.compile(r"(\d{8})")

          def yyyymmdd_from_name(name: str):
              m = date_re.search(name)
              return m.group(1) if m else None

          def load_json(path: Path):
              with open(path, "r", encoding="utf-8") as f:
                  return json.load(f)

          def flatten_meta(meta: dict):
              out = {}
              for k, v in meta.items():
                  if isinstance(v, (str, int, float, bool)) or v is None:
                      out["meta_" + k] = v
              return out

          meta_files = sorted(BRONZE.rglob("*.meta.json"))
          data_files = sorted(BRONZE.rglob("*.csv.gz"))

          meta_by_day = {}
          for p in meta_files:
              day = yyyymmdd_from_name(p.name)
              if day and day.startswith(only_month):
                  meta_by_day[day] = load_json(p)

          data_by_day = {}
          for p in data_files:
              day = yyyymmdd_from_name(p.name)
              if day and day.startswith(only_month):
                  data_by_day[day] = p

          common_days = sorted(set(meta_by_day.keys()) & set(data_by_day.keys()))
          print("month:", only_month, "days:", len(common_days))
          if not common_days:
              raise SystemExit("No days to process for month")

          year = only_month[:4]
          out_path = (SILVER / year / f"{only_month}.parquet").resolve()
          out_path.parent.mkdir(parents=True, exist_ok=True)

          frames = []
          for day in common_days:
              csv_path = data_by_day[day]
              meta = meta_by_day.get(day, {})
              df = pd.read_csv(csv_path, compression="gzip")
              df.columns = [str(c).lower().strip() for c in df.columns]
              df["ref_day"] = day
              df["__source_path"] = str(csv_path.relative_to(BRONZE))
              flat = flatten_meta(meta)
              for k, v in flat.items():
                  df[k] = v
              frames.append(df)

          out = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()
          out.to_parquet(out_path, index=False)
          print("written:", out_path, "rows:", len(out))
          PY

      - name: Silver days to silver trains for 202406
        run: |
          python - <<'PY'
          import ast
          import pandas as pd
          from pathlib import Path

          ROOT = Path(".").resolve()
          SILVER_DAYS = (ROOT / "data" / "silver").resolve()
          SILVER_TRAINS = (ROOT / "data" / "silver_trains").resolve()
          SILVER_TRAINS.mkdir(parents=True, exist_ok=True)

          only_month = "202406"
          year = only_month[:4]
          in_path = (SILVER_DAYS / year / f"{only_month}.parquet").resolve()
          if not in_path.exists():
              raise SystemExit(f"Missing {in_path}")

          df = pd.read_parquet(in_path)
          if "treni" not in df.columns:
              raise SystemExit("Missing column treni in silver days parquet")

          def parse_treni_cell(x):
              if x is None or not isinstance(x, str):
                  return []
              s = x.strip()
              if not s:
                  return []
              try:
                  obj = ast.literal_eval(s)
              except Exception:
                  return []
              return obj if isinstance(obj, list) else []

          frames = []
          skipped = 0
          for _, row in df.iterrows():
              treni_list = parse_treni_cell(row["treni"])
              if not treni_list:
                  skipped += 1
                  continue
              tdf = pd.json_normalize(treni_list)
              tdf["ref_day"] = row.get("ref_day")
              tdf["giorno"] = row.get("giorno")
              tdf["timezone"] = row.get("timezone")
              tdf["__source_path"] = row.get("__source_path")
              for c in df.columns:
                  if str(c).startswith("meta_"):
                      tdf[c] = row.get(c)
              frames.append(tdf)

          out = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()
          out_dir = (SILVER_TRAINS / year).resolve()
          out_dir.mkdir(parents=True, exist_ok=True)
          out_path = (out_dir / f"{only_month}.parquet").resolve()
          out.to_parquet(out_path, index=False)
          print("written:", out_path, "rows:", len(out), "skipped_days:", skipped)
          PY

      - name: Build gold for 202406 and merge into docs/data
        run: |
          python - <<'PY'
          import duckdb
          import pandas as pd
          from pathlib import Path

          ROOT = Path(".").resolve()
          SILVER_TRAINS = (ROOT / "data" / "silver_trains").resolve()
          DOCS = (ROOT / "docs" / "data").resolve()
          DOCS.mkdir(parents=True, exist_ok=True)

          only_month = "202406"
          year = only_month[:4]
          in_path = (SILVER_TRAINS / year / f"{only_month}.parquet").resolve()
          if not in_path.exists():
              raise SystemExit(f"Missing {in_path}")

          con = duckdb.connect()
          con.execute("DROP VIEW IF EXISTS trains_all")
          con.execute(f"CREATE VIEW trains_all AS SELECT * FROM read_parquet('{in_path.as_posix()}')")

          con.execute("DROP TABLE IF EXISTS trains_clean")
          con.execute("""
          CREATE TABLE trains_clean AS
          SELECT
            strptime(ref_day, '%Y%m%d')::DATE AS giorno,
            CASE
              WHEN ra IS NULL THEN NULL
              WHEN lower(trim(CAST(ra AS VARCHAR))) IN ('n','n.d.','nd','') THEN NULL
              ELSE TRY_CAST(ra AS DOUBLE)
            END AS ritardo_arrivo_min
          FROM trains_all
          """)

          kpi_giorno_new = con.execute("""
          SELECT
            giorno,
            COUNT(*) AS corse_osservate,
            AVG(ritardo_arrivo_min) AS ritardo_medio,
            quantile_cont(ritardo_arrivo_min, 0.5) AS ritardo_mediano,
            quantile_cont(ritardo_arrivo_min, 0.9) AS p90,
            SUM(CASE WHEN ritardo_arrivo_min > 5 THEN 1 ELSE 0 END) AS oltre_5,
            SUM(CASE WHEN ritardo_arrivo_min > 10 THEN 1 ELSE 0 END) AS oltre_10,
            SUM(CASE WHEN ritardo_arrivo_min > 15 THEN 1 ELSE 0 END) AS oltre_15,
            SUM(CASE WHEN ritardo_arrivo_min > 30 THEN 1 ELSE 0 END) AS oltre_30,
            SUM(CASE WHEN ritardo_arrivo_min > 60 THEN 1 ELSE 0 END) AS oltre_60
          FROM trains_clean
          GROUP BY 1
          ORDER BY 1
          """).fetchdf()

          kpi_mese_new = con.execute("""
          SELECT
            date_trunc('month', giorno)::DATE AS mese,
            COUNT(*) AS corse_osservate,
            AVG(ritardo_arrivo_min) AS ritardo_medio,
            quantile_cont(ritardo_arrivo_min, 0.5) AS ritardo_mediano,
            quantile_cont(ritardo_arrivo_min, 0.9) AS p90,
            SUM(CASE WHEN ritardo_arrivo_min > 5 THEN 1 ELSE 0 END) AS oltre_5,
            SUM(CASE WHEN ritardo_arrivo_min > 10 THEN 1 ELSE 0 END) AS oltre_10,
            SUM(CASE WHEN ritardo_arrivo_min > 15 THEN 1 ELSE 0 END) AS oltre_15,
            SUM(CASE WHEN ritardo_arrivo_min > 30 THEN 1 ELSE 0 END) AS oltre_30,
            SUM(CASE WHEN ritardo_arrivo_min > 60 THEN 1 ELSE 0 END) AS oltre_60
          FROM trains_clean
          GROUP BY 1
          ORDER BY 1
          """).fetchdf()

          con.close()

          def merge_csv(path: Path, new_df: pd.DataFrame, key: str):
              if path.exists():
                  old = pd.read_csv(path, parse_dates=[key])
                  merged = pd.concat([old, new_df], ignore_index=True)
              else:
                  merged = new_df.copy()
              merged[key] = pd.to_datetime(merged[key], errors="coerce")
              merged = merged.dropna(subset=[key])
              merged = merged.drop_duplicates(subset=[key], keep="last")
              merged = merged.sort_values(key)
              merged.to_csv(path, index=False)

          merge_csv(DOCS / "kpi_giorno.csv", kpi_giorno_new, "giorno")
          merge_csv(DOCS / "kpi_mese.csv", kpi_mese_new, "mese")

          (ROOT / "docs" / ".nojekyll").touch()
          print("merged into", DOCS)
          PY

      - name: Commit and push docs/data
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add docs/data docs/.nojekyll
          git commit -m "One shot merge gold 2024-06" || echo "No changes"
          git push
```
