name: One shot 2024-06 (days -> trains -> KPI merge)

on:
  workflow_dispatch:

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install pandas pyarrow duckdb

      - name: Bronze days to silver days (202406)
        run: |
          python - <<'PY'
          import json, re
          import pandas as pd
          from pathlib import Path

          ROOT = Path(".").resolve()
          BRONZE = ROOT / "data" / "bronze"
          SILVER = ROOT / "data" / "silver"
          SILVER.mkdir(parents=True, exist_ok=True)

          only_month = "202406"
          date_re = re.compile(r"(\d{8})")

          def yyyymmdd_from_name(name: str):
              m = date_re.search(name)
              return m.group(1) if m else None

          def load_json(path: Path):
              with open(path, "r", encoding="utf-8") as f:
                  return json.load(f)

          def flatten_meta(meta: dict):
              out = {}
              for k, v in meta.items():
                  if isinstance(v, (str, int, float, bool)) or v is None:
                      out["meta_" + k] = v
              return out

          meta_files = sorted(BRONZE.rglob("*.meta.json"))
          data_files = sorted(BRONZE.rglob("*.csv.gz"))

          meta_by_day = {}
          for p in meta_files:
              day = yyyymmdd_from_name(p.name)
              if day and day.startswith(only_month):
                  meta_by_day[day] = load_json(p)

          data_by_day = {}
          for p in data_files:
              day = yyyymmdd_from_name(p.name)
              if day and day.startswith(only_month):
                  data_by_day[day] = p

          common_days = sorted(set(meta_by_day.keys()) & set(data_by_day.keys()))
          print("month:", only_month, "days:", len(common_days))
          if not common_days:
              raise SystemExit("No days found for month")

          frames = []
          for day in common_days:
              csv_path = data_by_day[day]
              meta = meta_by_day.get(day, {})
              df = pd.read_csv(csv_path, compression="gzip")
              df.columns = [str(c).lower().strip() for c in df.columns]
              df["ref_day"] = day
              df["__source_path"] = str(csv_path)
              flat = flatten_meta(meta)
              for k, v in flat.items():
                  df[k] = v
              frames.append(df)

          out = pd.concat(frames, ignore_index=True)
          year = only_month[:4]
          out_path = (SILVER / year / f"{only_month}.parquet").resolve()
          out_path.parent.mkdir(parents=True, exist_ok=True)
          out.to_parquet(out_path, index=False)
          print("written:", out_path, "rows:", len(out))
          PY

      - name: Silver days to silver trains (202406)
        run: |
          python - <<'PY'
          import ast
          import pandas as pd
          from pathlib import Path

          ROOT = Path(".").resolve()
          SILVER_DAYS = ROOT / "data" / "silver"
          SILVER_TRAINS = ROOT / "data" / "silver_trains"
          SILVER_TRAINS.mkdir(parents=True, exist_ok=True)

          only_month = "202406"
          year = only_month[:4]
          in_path = (SILVER_DAYS / year / f"{only_month}.parquet").resolve()
          if not in_path.exists():
              raise SystemExit(f"Missing {in_path}")

          df = pd.read_parquet(in_path)
          if "treni" not in df.columns:
              raise SystemExit("Missing column treni in silver days parquet")

          def parse_treni_cell(x):
              if x is None or not isinstance(x, str):
                  return []
              s = x.strip()
              if not s:
                  return []
              try:
                  obj = ast.literal_eval(s)
              except Exception:
                  return []
              return obj if isinstance(obj, list) else []

          frames = []
          skipped = 0
          for _, row in df.iterrows():
              treni_list = parse_treni_cell(row["treni"])
              if not treni_list:
                  skipped += 1
                  continue
              tdf = pd.json_normalize(treni_list)
              tdf["ref_day"] = row.get("ref_day")
              tdf["giorno"] = row.get("giorno")
              tdf["timezone"] = row.get("timezone")
              for c in df.columns:
                  if str(c).startswith("meta_"):
                      tdf[c] = row.get(c)
              frames.append(tdf)

          out = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()
          out_dir = (SILVER_TRAINS / year).resolve()
          out_dir.mkdir(parents=True, exist_ok=True)
          out_path = (out_dir / f"{only_month}.parquet").resolve()
          out.to_parquet(out_path, index=False)
          print("written:", out_path, "rows:", len(out), "skipped_days:", skipped)
          PY

      - name: Build KPI day/month + by category and merge into docs/data
        run: |
          python - <<'PY'
          from pathlib import Path
          import duckdb
          import pandas as pd

          ROOT = Path(".").resolve()
          DOCS = ROOT / "docs" / "data"
          DOCS.mkdir(parents=True, exist_ok=True)

          only_month = "202406"
          year = only_month[:4]
          trains_path = (ROOT / "data" / "silver_trains" / year / f"{only_month}.parquet").resolve()
          if not trains_path.exists():
              raise SystemExit(f"Missing {trains_path}")

          con = duckdb.connect()
          con.execute("DROP VIEW IF EXISTS trains_all")
          con.execute(f"CREATE VIEW trains_all AS SELECT * FROM read_parquet('{trains_path.as_posix()}')")

          con.execute("DROP TABLE IF EXISTS trains_clean")
          con.execute("""
          CREATE TABLE trains_clean AS
          SELECT
            strptime(ref_day, '%Y%m%d')::DATE AS giorno,
            COALESCE(CAST(c AS VARCHAR), 'UNK') AS categoria,
            CASE
              WHEN ra IS NULL THEN NULL
              WHEN lower(trim(CAST(ra AS VARCHAR))) IN ('n','n.d.','nd','') THEN NULL
              ELSE TRY_CAST(ra AS DOUBLE)
            END AS ra_min
          FROM trains_all
          """)

          kpi_giorno_new = con.execute("""
          SELECT
            giorno,
            COUNT(*) AS corse_osservate,
            COUNT(ra_min) AS effettuate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS cancellate,
            0 AS soppresse,
            0 AS parzialmente_cancellate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS info_mancante,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min = 0 THEN 1 ELSE 0 END) AS in_orario,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min > 0 THEN 1 ELSE 0 END) AS in_ritardo,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min < 0 THEN 1 ELSE 0 END) AS in_anticipo,
            SUM(CASE WHEN ra_min > 5 THEN 1 ELSE 0 END) AS oltre_5,
            SUM(CASE WHEN ra_min > 10 THEN 1 ELSE 0 END) AS oltre_10,
            SUM(CASE WHEN ra_min > 15 THEN 1 ELSE 0 END) AS oltre_15,
            SUM(CASE WHEN ra_min > 30 THEN 1 ELSE 0 END) AS oltre_30,
            SUM(CASE WHEN ra_min > 60 THEN 1 ELSE 0 END) AS oltre_60,
            SUM(CASE WHEN ra_min > 0 THEN ra_min ELSE 0 END) AS minuti_ritardo_tot,
            SUM(CASE WHEN ra_min < 0 THEN -ra_min ELSE 0 END) AS minuti_anticipo_tot,
            SUM(COALESCE(ra_min, 0)) AS minuti_netti_tot,
            AVG(ra_min) AS ritardo_medio,
            quantile_cont(ra_min, 0.5) AS ritardo_mediano,
            quantile_cont(ra_min, 0.9) AS p90,
            quantile_cont(ra_min, 0.95) AS p95
          FROM trains_clean
          GROUP BY 1
          ORDER BY 1
          """).fetchdf()

          kpi_mese_new = con.execute("""
          SELECT
            date_trunc('month', giorno)::DATE AS mese,
            COUNT(*) AS corse_osservate,
            COUNT(ra_min) AS effettuate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS cancellate,
            0 AS soppresse,
            0 AS parzialmente_cancellate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS info_mancante,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min = 0 THEN 1 ELSE 0 END) AS in_orario,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min > 0 THEN 1 ELSE 0 END) AS in_ritardo,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min < 0 THEN 1 ELSE 0 END) AS in_anticipo,
            SUM(CASE WHEN ra_min > 5 THEN 1 ELSE 0 END) AS oltre_5,
            SUM(CASE WHEN ra_min > 10 THEN 1 ELSE 0 END) AS oltre_10,
            SUM(CASE WHEN ra_min > 15 THEN 1 ELSE 0 END) AS oltre_15,
            SUM(CASE WHEN ra_min > 30 THEN 1 ELSE 0 END) AS oltre_30,
            SUM(CASE WHEN ra_min > 60 THEN 1 ELSE 0 END) AS oltre_60,
            SUM(CASE WHEN ra_min > 0 THEN ra_min ELSE 0 END) AS minuti_ritardo_tot,
            SUM(CASE WHEN ra_min < 0 THEN -ra_min ELSE 0 END) AS minuti_anticipo_tot,
            SUM(COALESCE(ra_min, 0)) AS minuti_netti_tot,
            AVG(ra_min) AS ritardo_medio,
            quantile_cont(ra_min, 0.5) AS ritardo_mediano,
            quantile_cont(ra_min, 0.9) AS p90,
            quantile_cont(ra_min, 0.95) AS p95
          FROM trains_clean
          GROUP BY 1
          ORDER BY 1
          """).fetchdf()

          kpi_giorno_cat_new = con.execute("""
          SELECT
            giorno,
            categoria,
            COUNT(*) AS corse_osservate,
            COUNT(ra_min) AS effettuate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS cancellate,
            0 AS soppresse,
            0 AS parzialmente_cancellate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS info_mancante,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min = 0 THEN 1 ELSE 0 END) AS in_orario,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min > 0 THEN 1 ELSE 0 END) AS in_ritardo,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min < 0 THEN 1 ELSE 0 END) AS in_anticipo,
            SUM(CASE WHEN ra_min > 5 THEN 1 ELSE 0 END) AS oltre_5,
            SUM(CASE WHEN ra_min > 10 THEN 1 ELSE 0 END) AS oltre_10,
            SUM(CASE WHEN ra_min > 15 THEN 1 ELSE 0 END) AS oltre_15,
            SUM(CASE WHEN ra_min > 30 THEN 1 ELSE 0 END) AS oltre_30,
            SUM(CASE WHEN ra_min > 60 THEN 1 ELSE 0 END) AS oltre_60,
            SUM(CASE WHEN ra_min > 0 THEN ra_min ELSE 0 END) AS minuti_ritardo_tot,
            SUM(CASE WHEN ra_min < 0 THEN -ra_min ELSE 0 END) AS minuti_anticipo_tot,
            SUM(COALESCE(ra_min, 0)) AS minuti_netti_tot,
            AVG(ra_min) AS ritardo_medio,
            quantile_cont(ra_min, 0.5) AS ritardo_mediano,
            quantile_cont(ra_min, 0.9) AS p90,
            quantile_cont(ra_min, 0.95) AS p95
          FROM trains_clean
          GROUP BY 1,2
          ORDER BY 1,2
          """).fetchdf()

          kpi_mese_cat_new = con.execute("""
          SELECT
            date_trunc('month', giorno)::DATE AS mese,
            categoria,
            COUNT(*) AS corse_osservate,
            COUNT(ra_min) AS effettuate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS cancellate,
            0 AS soppresse,
            0 AS parzialmente_cancellate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS info_mancante,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min = 0 THEN 1 ELSE 0 END) AS in_orario,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min > 0 THEN 1 ELSE 0 END) AS in_ritardo,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min < 0 THEN 1 ELSE 0 END) AS in_anticipo,
            SUM(CASE WHEN ra_min > 5 THEN 1 ELSE 0 END) AS oltre_5,
            SUM(CASE WHEN ra_min > 10 THEN 1 ELSE 0 END) AS oltre_10,
            SUM(CASE WHEN ra_min > 15 THEN 1 ELSE 0 END) AS oltre_15,
            SUM(CASE WHEN ra_min > 30 THEN 1 ELSE 0 END) AS oltre_30,
            SUM(CASE WHEN ra_min > 60 THEN 1 ELSE 0 END) AS oltre_60,
            SUM(CASE WHEN ra_min > 0 THEN ra_min ELSE 0 END) AS minuti_ritardo_tot,
            SUM(CASE WHEN ra_min < 0 THEN -ra_min ELSE 0 END) AS minuti_anticipo_tot,
            SUM(COALESCE(ra_min, 0)) AS minuti_netti_tot,
            AVG(ra_min) AS ritardo_medio,
            quantile_cont(ra_min, 0.5) AS ritardo_mediano,
            quantile_cont(ra_min, 0.9) AS p90,
            quantile_cont(ra_min, 0.95) AS p95
          FROM trains_clean
          GROUP BY 1,2
          ORDER BY 1,2
          """).fetchdf()

          con.close()

          def merge_csv(path: Path, new_df: pd.DataFrame, key_cols):
              if path.exists():
                  old = pd.read_csv(path)
                  df = pd.concat([old, new_df], ignore_index=True)
                  df = df.drop_duplicates(subset=key_cols, keep="last")
              else:
                  df = new_df
              try:
                  df = df.sort_values(key_cols)
              except Exception:
                  pass
              df.to_csv(path, index=False)

          merge_csv(DOCS / "kpi_giorno.csv", kpi_giorno_new, ["giorno"])
          merge_csv(DOCS / "kpi_mese.csv", kpi_mese_new, ["mese"])
          merge_csv(DOCS / "kpi_giorno_categoria.csv", kpi_giorno_cat_new, ["giorno", "categoria"])
          merge_csv(DOCS / "kpi_mese_categoria.csv", kpi_mese_cat_new, ["mese", "categoria"])

          (ROOT / "docs" / ".nojekyll").touch()
          print("updated docs/data KPI files")
          PY

      - name: Commit and push published datasets
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add docs/data docs/.nojekyll
          git commit -m "Backfill 2024-06 KPI merge" || echo "No changes"
          git push
