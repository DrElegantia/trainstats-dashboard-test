name: One shot 2024-06 (days -> trains -> KPI + stazioni + OD -> merge)

on:
  workflow_dispatch:

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install pandas pyarrow duckdb

      - name: Bronze days to silver days (202406)
        run: |
          python - <<'PY'
          import json, re
          import pandas as pd
          from pathlib import Path

          ROOT = Path(".").resolve()
          BRONZE = ROOT / "data" / "bronze"
          SILVER = ROOT / "data" / "silver"
          SILVER.mkdir(parents=True, exist_ok=True)

          only_month = "202406"
          date_re = re.compile(r"(\d{8})")

          def yyyymmdd_from_name(name: str):
              m = date_re.search(name)
              return m.group(1) if m else None

          def load_json(path: Path):
              with open(path, "r", encoding="utf-8") as f:
                  return json.load(f)

          def flatten_meta(meta: dict):
              out = {}
              for k, v in meta.items():
                  if isinstance(v, (str, int, float, bool)) or v is None:
                      out["meta_" + k] = v
              return out

          meta_files = sorted(BRONZE.rglob("*.meta.json"))
          data_files = sorted(BRONZE.rglob("*.csv.gz"))

          meta_by_day = {}
          for p in meta_files:
              day = yyyymmdd_from_name(p.name)
              if day and day.startswith(only_month):
                  meta_by_day[day] = load_json(p)

          data_by_day = {}
          for p in data_files:
              day = yyyymmdd_from_name(p.name)
              if day and day.startswith(only_month):
                  data_by_day[day] = p

          common_days = sorted(set(meta_by_day.keys()) & set(data_by_day.keys()))
          print("month:", only_month, "days:", len(common_days))
          if not common_days:
              raise SystemExit("No days found for month")

          frames = []
          for day in common_days:
              csv_path = data_by_day[day]
              meta = meta_by_day.get(day, {})
              df = pd.read_csv(csv_path, compression="gzip")
              df.columns = [str(c).lower().strip() for c in df.columns]
              df["ref_day"] = day
              df["__source_path"] = str(csv_path)
              flat = flatten_meta(meta)
              for k, v in flat.items():
                  df[k] = v
              frames.append(df)

          out = pd.concat(frames, ignore_index=True)
          year = only_month[:4]
          out_path = (SILVER / year / f"{only_month}.parquet").resolve()
          out_path.parent.mkdir(parents=True, exist_ok=True)
          out.to_parquet(out_path, index=False)
          print("written:", out_path, "rows:", len(out))
          PY

      - name: Silver days to silver trains (202406)
        run: |
          python - <<'PY'
          import ast
          import pandas as pd
          from pathlib import Path

          ROOT = Path(".").resolve()
          SILVER_DAYS = ROOT / "data" / "silver"
          SILVER_TRAINS = ROOT / "data" / "silver_trains"
          SILVER_TRAINS.mkdir(parents=True, exist_ok=True)

          only_month = "202406"
          year = only_month[:4]
          in_path = (SILVER_DAYS / year / f"{only_month}.parquet").resolve()
          if not in_path.exists():
              raise SystemExit(f"Missing {in_path}")

          df = pd.read_parquet(in_path)
          if "treni" not in df.columns:
              raise SystemExit("Missing column treni in silver days parquet")

          def parse_treni_cell(x):
              if x is None or not isinstance(x, str):
                  return []
              s = x.strip()
              if not s:
                  return []
              try:
                  obj = ast.literal_eval(s)
              except Exception:
                  return []
              return obj if isinstance(obj, list) else []

          frames = []
          skipped = 0
          for _, row in df.iterrows():
              treni_list = parse_treni_cell(row["treni"])
              if not treni_list:
                  skipped += 1
                  continue
              tdf = pd.json_normalize(treni_list)
              tdf["ref_day"] = row.get("ref_day")
              tdf["giorno"] = row.get("giorno")
              tdf["timezone"] = row.get("timezone")
              for c in df.columns:
                  if str(c).startswith("meta_"):
                      tdf[c] = row.get(c)
              frames.append(tdf)

          out = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()
          out_dir = (SILVER_TRAINS / year).resolve()
          out_dir.mkdir(parents=True, exist_ok=True)
          out_path = (out_dir / f"{only_month}.parquet").resolve()
          out.to_parquet(out_path, index=False)
          print("written:", out_path, "rows:", len(out), "skipped_days:", skipped)
          PY

      - name: Build KPI + stazioni + OD and merge into docs/data
        run: |
          python - <<'PY'
          from pathlib import Path
          import duckdb
          import pandas as pd

          ROOT = Path(".").resolve()
          DOCS = ROOT / "docs" / "data"
          DOCS.mkdir(parents=True, exist_ok=True)

          only_month = "202406"
          year = only_month[:4]
          trains_path = (ROOT / "data" / "silver_trains" / year / f"{only_month}.parquet").resolve()
          if not trains_path.exists():
              raise SystemExit(f"Missing {trains_path}")

          def merge_csv(path: Path, new_df: pd.DataFrame, key_cols):
              for k in key_cols:
                  if k not in new_df.columns:
                      raise KeyError(f"merge_csv: missing key col {k} for {path.name}. have={list(new_df.columns)[:50]}")
              if path.exists():
                  old = pd.read_csv(path)
                  df = pd.concat([old, new_df], ignore_index=True)
                  for k in key_cols:
                      if k not in df.columns:
                          df[k] = pd.NA
                  df = df.drop_duplicates(subset=key_cols, keep="last")
              else:
                  df = new_df.drop_duplicates(subset=key_cols, keep="last")
              try:
                  df = df.sort_values(key_cols)
              except Exception:
                  pass
              df.to_csv(path, index=False)

          def align_to_existing_schema(target: Path, df: pd.DataFrame):
              if not target.exists():
                  return df
              cols = pd.read_csv(target, nrows=0).columns.tolist()
              for c in cols:
                  if c not in df.columns:
                      df[c] = pd.NA
              return df[cols]

          def harmonize_station_schema(target: Path, df: pd.DataFrame):
              if "stazione" in df.columns and "cod_stazione" not in df.columns:
                  df = df.rename(columns={"stazione": "cod_stazione"})
              if target.exists():
                  cols = pd.read_csv(target, nrows=0).columns.tolist()
                  if "stazione" in cols and "cod_stazione" in df.columns and "stazione" not in df.columns:
                      df["stazione"] = df["cod_stazione"]
                  if "cod_stazione" in cols and "stazione" in df.columns and "cod_stazione" not in df.columns:
                      df["cod_stazione"] = df["stazione"]
              return df

          con = duckdb.connect()
          con.execute("PRAGMA threads=4")
          con.execute("DROP VIEW IF EXISTS trains_all")
          con.execute(f"CREATE VIEW trains_all AS SELECT * FROM read_parquet('{trains_path.as_posix()}')")

          con.execute("DROP TABLE IF EXISTS trains_clean")
          con.execute("""
          CREATE TABLE trains_clean AS
          SELECT
            strptime(ref_day, '%Y%m%d')::DATE AS giorno,
            date_trunc('month', strptime(ref_day, '%Y%m%d')::DATE)::DATE AS mese,
            COALESCE(CAST(c AS VARCHAR), 'UNK') AS categoria,
            COALESCE(CAST(n AS VARCHAR), CAST(_id AS VARCHAR), 'UNK') AS numero_treno,
            COALESCE(CAST(p AS VARCHAR), 'UNK') AS origine,
            COALESCE(CAST(a AS VARCHAR), 'UNK') AS destinazione,
            CASE
              WHEN ra IS NULL THEN NULL
              WHEN lower(trim(CAST(ra AS VARCHAR))) IN ('n','n.d.','nd','') THEN NULL
              ELSE TRY_CAST(ra AS DOUBLE)
            END AS ra_min,
            fr
          FROM trains_all
          """)

          con.execute("DROP TABLE IF EXISTS stops_clean")
          con.execute("""
          CREATE TABLE stops_clean AS
          WITH base AS (
            SELECT
              giorno,
              mese,
              categoria,
              numero_treno,
              fr,
              array_length(fr) AS max_idx
            FROM trains_clean
            WHERE fr IS NOT NULL AND array_length(fr) > 0
          ),
          idxs AS (
            SELECT
              giorno,
              mese,
              categoria,
              numero_treno,
              max_idx,
              UNNEST(range(1, max_idx + 1)) AS idx
            FROM base
          ),
          s AS (
            SELECT
              base.giorno,
              base.mese,
              base.categoria,
              base.numero_treno,
              idxs.idx,
              base.max_idx,
              list_extract(base.fr, idxs.idx) AS stop
            FROM base
            JOIN idxs
              ON base.giorno = idxs.giorno
             AND base.mese = idxs.mese
             AND base.categoria = idxs.categoria
             AND base.numero_treno = idxs.numero_treno
             AND base.max_idx = idxs.max_idx
          )
          SELECT
            giorno,
            mese,
            categoria,
            numero_treno,
            CAST(stop.n AS VARCHAR) AS stazione,
            CASE
              WHEN idx = 1 THEN 'PARTENZA'
              WHEN idx = max_idx THEN 'ARRIVO'
              ELSE 'TRANSITO'
            END AS ruolo,
            CASE
              WHEN stop.ra IS NULL THEN NULL
              WHEN lower(trim(CAST(stop.ra AS VARCHAR))) IN ('n','n.d.','nd','') THEN NULL
              ELSE TRY_CAST(stop.ra AS DOUBLE)
            END AS ra_min
          FROM s
          WHERE stop.n IS NOT NULL
          """)

          kpi_giorno = con.execute("""
          SELECT
            giorno,
            COUNT(*) AS corse_osservate,
            COUNT(ra_min) AS effettuate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS cancellate,
            0 AS soppresse,
            0 AS parzialmente_cancellate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS info_mancante,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min = 0 THEN 1 ELSE 0 END) AS in_orario,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min > 0 THEN 1 ELSE 0 END) AS in_ritardo,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min < 0 THEN 1 ELSE 0 END) AS in_anticipo,
            SUM(CASE WHEN ra_min > 5 THEN 1 ELSE 0 END) AS oltre_5,
            SUM(CASE WHEN ra_min > 10 THEN 1 ELSE 0 END) AS oltre_10,
            SUM(CASE WHEN ra_min > 15 THEN 1 ELSE 0 END) AS oltre_15,
            SUM(CASE WHEN ra_min > 30 THEN 1 ELSE 0 END) AS oltre_30,
            SUM(CASE WHEN ra_min > 60 THEN 1 ELSE 0 END) AS oltre_60,
            SUM(CASE WHEN ra_min > 0 THEN ra_min ELSE 0 END) AS minuti_ritardo_tot,
            SUM(CASE WHEN ra_min < 0 THEN -ra_min ELSE 0 END) AS minuti_anticipo_tot,
            SUM(COALESCE(ra_min, 0)) AS minuti_netti_tot,
            AVG(ra_min) AS ritardo_medio,
            quantile_cont(ra_min, 0.5) AS ritardo_mediano,
            quantile_cont(ra_min, 0.9) AS p90,
            quantile_cont(ra_min, 0.95) AS p95
          FROM trains_clean
          GROUP BY 1
          ORDER BY 1
          """).fetchdf()

          kpi_mese = con.execute("""
          SELECT
            mese,
            COUNT(*) AS corse_osservate,
            COUNT(ra_min) AS effettuate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS cancellate,
            0 AS soppresse,
            0 AS parzialmente_cancellate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS info_mancante,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min = 0 THEN 1 ELSE 0 END) AS in_orario,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min > 0 THEN 1 ELSE 0 END) AS in_ritardo,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min < 0 THEN 1 ELSE 0 END) AS in_anticipo,
            SUM(CASE WHEN ra_min > 5 THEN 1 ELSE 0 END) AS oltre_5,
            SUM(CASE WHEN ra_min > 10 THEN 1 ELSE 0 END) AS oltre_10,
            SUM(CASE WHEN ra_min > 15 THEN 1 ELSE 0 END) AS oltre_15,
            SUM(CASE WHEN ra_min > 30 THEN 1 ELSE 0 END) AS oltre_30,
            SUM(CASE WHEN ra_min > 60 THEN 1 ELSE 0 END) AS oltre_60,
            SUM(CASE WHEN ra_min > 0 THEN ra_min ELSE 0 END) AS minuti_ritardo_tot,
            SUM(CASE WHEN ra_min < 0 THEN -ra_min ELSE 0 END) AS minuti_anticipo_tot,
            SUM(COALESCE(ra_min, 0)) AS minuti_netti_tot,
            AVG(ra_min) AS ritardo_medio,
            quantile_cont(ra_min, 0.5) AS ritardo_mediano,
            quantile_cont(ra_min, 0.9) AS p90,
            quantile_cont(ra_min, 0.95) AS p95
          FROM trains_clean
          GROUP BY 1
          ORDER BY 1
          """).fetchdf()

          kpi_giorno_cat = con.execute("""
          SELECT
            giorno,
            categoria,
            COUNT(*) AS corse_osservate,
            COUNT(ra_min) AS effettuate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS cancellate,
            0 AS soppresse,
            0 AS parzialmente_cancellate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS info_mancante,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min = 0 THEN 1 ELSE 0 END) AS in_orario,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min > 0 THEN 1 ELSE 0 END) AS in_ritardo,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min < 0 THEN 1 ELSE 0 END) AS in_anticipo,
            SUM(CASE WHEN ra_min > 5 THEN 1 ELSE 0 END) AS oltre_5,
            SUM(CASE WHEN ra_min > 10 THEN 1 ELSE 0 END) AS oltre_10,
            SUM(CASE WHEN ra_min > 15 THEN 1 ELSE 0 END) AS oltre_15,
            SUM(CASE WHEN ra_min > 30 THEN 1 ELSE 0 END) AS oltre_30,
            SUM(CASE WHEN ra_min > 60 THEN 1 ELSE 0 END) AS oltre_60,
            SUM(CASE WHEN ra_min > 0 THEN ra_min ELSE 0 END) AS minuti_ritardo_tot,
            SUM(CASE WHEN ra_min < 0 THEN -ra_min ELSE 0 END) AS minuti_anticipo_tot,
            SUM(COALESCE(ra_min, 0)) AS minuti_netti_tot,
            AVG(ra_min) AS ritardo_medio,
            quantile_cont(ra_min, 0.5) AS ritardo_mediano,
            quantile_cont(ra_min, 0.9) AS p90,
            quantile_cont(ra_min, 0.95) AS p95
          FROM trains_clean
          GROUP BY 1,2
          ORDER BY 1,2
          """).fetchdf()

          kpi_mese_cat = con.execute("""
          SELECT
            mese,
            categoria,
            COUNT(*) AS corse_osservate,
            COUNT(ra_min) AS effettuate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS cancellate,
            0 AS soppresse,
            0 AS parzialmente_cancellate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS info_mancante,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min = 0 THEN 1 ELSE 0 END) AS in_orario,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min > 0 THEN 1 ELSE 0 END) AS in_ritardo,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min < 0 THEN 1 ELSE 0 END) AS in_anticipo,
            SUM(CASE WHEN ra_min > 5 THEN 1 ELSE 0 END) AS oltre_5,
            SUM(CASE WHEN ra_min > 10 THEN 1 ELSE 0 END) AS oltre_10,
            SUM(CASE WHEN ra_min > 15 THEN 1 ELSE 0 END) AS oltre_15,
            SUM(CASE WHEN ra_min > 30 THEN 1 ELSE 0 END) AS oltre_30,
            SUM(CASE WHEN ra_min > 60 THEN 1 ELSE 0 END) AS oltre_60,
            SUM(CASE WHEN ra_min > 0 THEN ra_min ELSE 0 END) AS minuti_ritardo_tot,
            SUM(CASE WHEN ra_min < 0 THEN -ra_min ELSE 0 END) AS minuti_anticipo_tot,
            SUM(COALESCE(ra_min, 0)) AS minuti_netti_tot,
            AVG(ra_min) AS ritardo_medio,
            quantile_cont(ra_min, 0.5) AS ritardo_mediano,
            quantile_cont(ra_min, 0.9) AS p90,
            quantile_cont(ra_min, 0.95) AS p95
          FROM trains_clean
          GROUP BY 1,2
          ORDER BY 1,2
          """).fetchdf()

          st_nodo = con.execute("""
          SELECT
            mese,
            categoria,
            stazione,
            COUNT(*) AS corse_osservate,
            COUNT(ra_min) AS effettuate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS cancellate,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min = 0 THEN 1 ELSE 0 END) AS in_orario,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min > 0 THEN 1 ELSE 0 END) AS in_ritardo,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min < 0 THEN 1 ELSE 0 END) AS in_anticipo,
            AVG(ra_min) AS ritardo_medio,
            quantile_cont(ra_min, 0.5) AS ritardo_mediano,
            quantile_cont(ra_min, 0.9) AS p90,
            quantile_cont(ra_min, 0.95) AS p95
          FROM stops_clean
          GROUP BY 1,2,3
          ORDER BY 1,2,3
          """).fetchdf()

          st_ruolo = con.execute("""
          SELECT
            mese,
            categoria,
            ruolo,
            stazione,
            COUNT(*) AS corse_osservate,
            COUNT(ra_min) AS effettuate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS cancellate,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min = 0 THEN 1 ELSE 0 END) AS in_orario,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min > 0 THEN 1 ELSE 0 END) AS in_ritardo,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min < 0 THEN 1 ELSE 0 END) AS in_anticipo,
            AVG(ra_min) AS ritardo_medio,
            quantile_cont(ra_min, 0.5) AS ritardo_mediano,
            quantile_cont(ra_min, 0.9) AS p90,
            quantile_cont(ra_min, 0.95) AS p95
          FROM stops_clean
          GROUP BY 1,2,3,4
          ORDER BY 1,2,3,4
          """).fetchdf()

          od = con.execute("""
          SELECT
            mese,
            categoria,
            origine,
            destinazione,
            COUNT(*) AS corse_osservate,
            COUNT(ra_min) AS effettuate,
            AVG(ra_min) AS ritardo_medio,
            quantile_cont(ra_min, 0.9) AS p90
          FROM trains_clean
          GROUP BY 1,2,3,4
          ORDER BY 1,2,3,4
          """).fetchdf()

          con.close()

          st_nodo = harmonize_station_schema(DOCS / "stazioni_mese_categoria_nodo.csv", st_nodo)
          st_ruolo = harmonize_station_schema(DOCS / "stazioni_mese_categoria_ruolo.csv", st_ruolo)

          merge_csv(DOCS / "kpi_giorno.csv", align_to_existing_schema(DOCS / "kpi_giorno.csv", kpi_giorno), ["giorno"])
          merge_csv(DOCS / "kpi_mese.csv", align_to_existing_schema(DOCS / "kpi_mese.csv", kpi_mese), ["mese"])
          merge_csv(DOCS / "kpi_giorno_categoria.csv", align_to_existing_schema(DOCS / "kpi_giorno_categoria.csv", kpi_giorno_cat), ["giorno", "categoria"])
          merge_csv(DOCS / "kpi_mese_categoria.csv", align_to_existing_schema(DOCS / "kpi_mese_categoria.csv", kpi_mese_cat), ["mese", "categoria"])

          merge_csv(
            DOCS / "stazioni_mese_categoria_nodo.csv",
            align_to_existing_schema(DOCS / "stazioni_mese_categoria_nodo.csv", st_nodo),
            ["mese", "categoria", "cod_stazione"]
          )

          merge_csv(
            DOCS / "stazioni_mese_categoria_ruolo.csv",
            align_to_existing_schema(DOCS / "stazioni_mese_categoria_ruolo.csv", st_ruolo),
            ["mese", "categoria", "ruolo", "cod_stazione"]
          )

          merge_csv(DOCS / "od_mese_categoria.csv", align_to_existing_schema(DOCS / "od_mese_categoria.csv", od), ["mese", "categoria", "origine", "destinazione"])

          (ROOT / "docs" / ".nojekyll").touch()
          print("updated docs/data")
          PY

      - name: Commit and push published datasets
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add docs/data docs/.nojekyll
          git commit -m "Backfill 2024-06 (KPI + stazioni + OD) merge" || echo "No changes"
          git push
