name: One shot 2024-06 (days -> trains -> gold extras -> merge)

on:
  workflow_dispatch:

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install pandas pyarrow duckdb

      - name: Bronze days to silver days (202406)
        run: |
          python - <<'PY'
          import json, re
          import pandas as pd
          from pathlib import Path

          ROOT = Path(".").resolve()
          BRONZE = ROOT / "data" / "bronze"
          SILVER = ROOT / "data" / "silver"
          SILVER.mkdir(parents=True, exist_ok=True)

          only_month = "202406"
          date_re = re.compile(r"(\d{8})")

          def yyyymmdd_from_name(name: str):
              m = date_re.search(name)
              return m.group(1) if m else None

          def load_json(path: Path):
              with open(path, "r", encoding="utf-8") as f:
                  return json.load(f)

          def flatten_meta(meta: dict):
              out = {}
              for k, v in meta.items():
                  if isinstance(v, (str, int, float, bool)) or v is None:
                      out["meta_" + k] = v
              return out

          meta_files = sorted(BRONZE.rglob("*.meta.json"))
          data_files = sorted(BRONZE.rglob("*.csv.gz"))

          meta_by_day = {}
          for p in meta_files:
              day = yyyymmdd_from_name(p.name)
              if day and day.startswith(only_month):
                  meta_by_day[day] = load_json(p)

          data_by_day = {}
          for p in data_files:
              day = yyyymmdd_from_name(p.name)
              if day and day.startswith(only_month):
                  data_by_day[day] = p

          common_days = sorted(set(meta_by_day.keys()) & set(data_by_day.keys()))
          print("month:", only_month, "days:", len(common_days))
          if not common_days:
              raise SystemExit("No days found for month")

          frames = []
          for day in common_days:
              csv_path = data_by_day[day]
              meta = meta_by_day.get(day, {})
              df = pd.read_csv(csv_path, compression="gzip")
              df.columns = [str(c).lower().strip() for c in df.columns]
              df["ref_day"] = day
              df["__source_path"] = str(csv_path)
              flat = flatten_meta(meta)
              for k, v in flat.items():
                  df[k] = v
              frames.append(df)

          out = pd.concat(frames, ignore_index=True)
          year = only_month[:4]
          out_path = (SILVER / year / f"{only_month}.parquet").resolve()
          out_path.parent.mkdir(parents=True, exist_ok=True)
          out.to_parquet(out_path, index=False)
          print("written:", out_path, "rows:", len(out))
          PY

      - name: Silver days to silver trains (202406)
        run: |
          python - <<'PY'
          import ast
          import pandas as pd
          from pathlib import Path

          ROOT = Path(".").resolve()
          SILVER_DAYS = ROOT / "data" / "silver"
          SILVER_TRAINS = ROOT / "data" / "silver_trains"
          SILVER_TRAINS.mkdir(parents=True, exist_ok=True)

          only_month = "202406"
          year = only_month[:4]
          in_path = (SILVER_DAYS / year / f"{only_month}.parquet").resolve()
          if not in_path.exists():
              raise SystemExit(f"Missing {in_path}")

          df = pd.read_parquet(in_path)
          if "treni" not in df.columns:
              raise SystemExit("Missing column treni in silver days parquet")

          def parse_treni_cell(x):
              if x is None or not isinstance(x, str):
                  return []
              s = x.strip()
              if not s:
                  return []
              try:
                  obj = ast.literal_eval(s)
              except Exception:
                  return []
              return obj if isinstance(obj, list) else []

          frames = []
          skipped = 0
          for _, row in df.iterrows():
              treni_list = parse_treni_cell(row["treni"])
              if not treni_list:
                  skipped += 1
                  continue
              tdf = pd.json_normalize(treni_list)
              tdf["ref_day"] = row.get("ref_day")
              tdf["giorno"] = row.get("giorno")
              tdf["timezone"] = row.get("timezone")
              for c in df.columns:
                  if str(c).startswith("meta_"):
                      tdf[c] = row.get(c)
              frames.append(tdf)

          out = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()
          out_dir = (SILVER_TRAINS / year).resolve()
          out_dir.mkdir(parents=True, exist_ok=True)
          out_path = (out_dir / f"{only_month}.parquet").resolve()
          out.to_parquet(out_path, index=False)
          print("written:", out_path, "rows:", len(out), "skipped_days:", skipped)
          PY

      - name: Build KPI + stations + distributions + OD and merge into docs/data
        run: |
          python - <<'PY'
          from pathlib import Path
          import duckdb
          import pandas as pd

          ROOT = Path(".").resolve()
          DOCS = ROOT / "docs" / "data"
          DOCS.mkdir(parents=True, exist_ok=True)

          only_month = "202406"
          year = only_month[:4]
          trains_path = (ROOT / "data" / "silver_trains" / year / f"{only_month}.parquet").resolve()
          if not trains_path.exists():
              raise SystemExit(f"Missing {trains_path}")

          def merge_csv(path: Path, new_df: pd.DataFrame, key_cols):
              if path.exists():
                  old = pd.read_csv(path)
                  df = pd.concat([old, new_df], ignore_index=True)
                  df = df.drop_duplicates(subset=key_cols, keep="last")
              else:
                  df = new_df
              try:
                  df = df.sort_values(key_cols)
              except Exception:
                  pass
              df.to_csv(path, index=False)

          def align_to_existing_schema(target: Path, df: pd.DataFrame):
              if not target.exists():
                  return df
              cols = pd.read_csv(target, nrows=0).columns.tolist()
              for c in cols:
                  if c not in df.columns:
                      df[c] = pd.NA
              df = df[cols]
              return df

          con = duckdb.connect()
          con.execute("PRAGMA threads=4")
          con.execute("DROP VIEW IF EXISTS trains_all")
          con.execute(f"CREATE VIEW trains_all AS SELECT * FROM read_parquet('{trains_path.as_posix()}')")

          con.execute("DROP TABLE IF EXISTS trains_clean")
          con.execute("""
          CREATE TABLE trains_clean AS
          SELECT
            strptime(ref_day, '%Y%m%d')::DATE AS giorno,
            date_trunc('month', strptime(ref_day, '%Y%m%d')::DATE)::DATE AS mese,
            COALESCE(CAST(c AS VARCHAR), 'UNK') AS categoria,
            COALESCE(CAST(n AS VARCHAR), CAST(numero AS VARCHAR), CAST(train AS VARCHAR), CAST(_id AS VARCHAR), 'UNK') AS numero_treno,
            COALESCE(CAST(p AS VARCHAR), 'UNK') AS origine,
            COALESCE(CAST(a AS VARCHAR), 'UNK') AS destinazione,
            CASE
              WHEN ra IS NULL THEN NULL
              WHEN lower(trim(CAST(ra AS VARCHAR))) IN ('n','n.d.','nd','') THEN NULL
              ELSE TRY_CAST(ra AS DOUBLE)
            END AS ra_min,
            fr
          FROM trains_all
          """)

          con.execute("DROP TABLE IF EXISTS stops_clean")
          con.execute("""
          CREATE TABLE stops_clean AS
          WITH base AS (
            SELECT
              giorno,
              mese,
              categoria,
              numero_treno,
              UNNEST(fr) WITH ORDINALITY AS s(stop, idx)
            FROM trains_clean
          ),
          w AS (
            SELECT
              giorno,
              mese,
              categoria,
              numero_treno,
              idx,
              MAX(idx) OVER (PARTITION BY giorno, categoria, numero_treno) AS max_idx,
              stop
            FROM base
          )
          SELECT
            giorno,
            mese,
            categoria,
            numero_treno,
            CAST(stop.n AS VARCHAR) AS stazione,
            CASE
              WHEN idx = 1 THEN 'PARTENZA'
              WHEN idx = max_idx THEN 'ARRIVO'
              ELSE 'TRANSITO'
            END AS ruolo,
            CASE
              WHEN stop.ra IS NULL THEN NULL
              WHEN lower(trim(CAST(stop.ra AS VARCHAR))) IN ('n','n.d.','nd','') THEN NULL
              ELSE TRY_CAST(stop.ra AS DOUBLE)
            END AS ra_min
          FROM w
          WHERE stop.n IS NOT NULL
          """)

          kpi_giorno = con.execute("""
          SELECT
            giorno,
            COUNT(*) AS corse_osservate,
            COUNT(ra_min) AS effettuate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS cancellate,
            0 AS soppresse,
            0 AS parzialmente_cancellate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS info_mancante,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min = 0 THEN 1 ELSE 0 END) AS in_orario,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min > 0 THEN 1 ELSE 0 END) AS in_ritardo,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min < 0 THEN 1 ELSE 0 END) AS in_anticipo,
            SUM(CASE WHEN ra_min > 5 THEN 1 ELSE 0 END) AS oltre_5,
            SUM(CASE WHEN ra_min > 10 THEN 1 ELSE 0 END) AS oltre_10,
            SUM(CASE WHEN ra_min > 15 THEN 1 ELSE 0 END) AS oltre_15,
            SUM(CASE WHEN ra_min > 30 THEN 1 ELSE 0 END) AS oltre_30,
            SUM(CASE WHEN ra_min > 60 THEN 1 ELSE 0 END) AS oltre_60,
            SUM(CASE WHEN ra_min > 0 THEN ra_min ELSE 0 END) AS minuti_ritardo_tot,
            SUM(CASE WHEN ra_min < 0 THEN -ra_min ELSE 0 END) AS minuti_anticipo_tot,
            SUM(COALESCE(ra_min, 0)) AS minuti_netti_tot,
            AVG(ra_min) AS ritardo_medio,
            quantile_cont(ra_min, 0.5) AS ritardo_mediano,
            quantile_cont(ra_min, 0.9) AS p90,
            quantile_cont(ra_min, 0.95) AS p95
          FROM trains_clean
          GROUP BY 1
          ORDER BY 1
          """).fetchdf()

          kpi_mese = con.execute("""
          SELECT
            mese,
            COUNT(*) AS corse_osservate,
            COUNT(ra_min) AS effettuate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS cancellate,
            0 AS soppresse,
            0 AS parzialmente_cancellate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS info_mancante,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min = 0 THEN 1 ELSE 0 END) AS in_orario,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min > 0 THEN 1 ELSE 0 END) AS in_ritardo,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min < 0 THEN 1 ELSE 0 END) AS in_anticipo,
            SUM(CASE WHEN ra_min > 5 THEN 1 ELSE 0 END) AS oltre_5,
            SUM(CASE WHEN ra_min > 10 THEN 1 ELSE 0 END) AS oltre_10,
            SUM(CASE WHEN ra_min > 15 THEN 1 ELSE 0 END) AS oltre_15,
            SUM(CASE WHEN ra_min > 30 THEN 1 ELSE 0 END) AS oltre_30,
            SUM(CASE WHEN ra_min > 60 THEN 1 ELSE 0 END) AS oltre_60,
            SUM(CASE WHEN ra_min > 0 THEN ra_min ELSE 0 END) AS minuti_ritardo_tot,
            SUM(CASE WHEN ra_min < 0 THEN -ra_min ELSE 0 END) AS minuti_anticipo_tot,
            SUM(COALESCE(ra_min, 0)) AS minuti_netti_tot,
            AVG(ra_min) AS ritardo_medio,
            quantile_cont(ra_min, 0.5) AS ritardo_mediano,
            quantile_cont(ra_min, 0.9) AS p90,
            quantile_cont(ra_min, 0.95) AS p95
          FROM trains_clean
          GROUP BY 1
          ORDER BY 1
          """).fetchdf()

          kpi_giorno_cat = con.execute("""
          SELECT
            giorno,
            categoria,
            COUNT(*) AS corse_osservate,
            COUNT(ra_min) AS effettuate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS cancellate,
            0 AS soppresse,
            0 AS parzialmente_cancellate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS info_mancante,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min = 0 THEN 1 ELSE 0 END) AS in_orario,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min > 0 THEN 1 ELSE 0 END) AS in_ritardo,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min < 0 THEN 1 ELSE 0 END) AS in_anticipo,
            SUM(CASE WHEN ra_min > 5 THEN 1 ELSE 0 END) AS oltre_5,
            SUM(CASE WHEN ra_min > 10 THEN 1 ELSE 0 END) AS oltre_10,
            SUM(CASE WHEN ra_min > 15 THEN 1 ELSE 0 END) AS oltre_15,
            SUM(CASE WHEN ra_min > 30 THEN 1 ELSE 0 END) AS oltre_30,
            SUM(CASE WHEN ra_min > 60 THEN 1 ELSE 0 END) AS oltre_60,
            SUM(CASE WHEN ra_min > 0 THEN ra_min ELSE 0 END) AS minuti_ritardo_tot,
            SUM(CASE WHEN ra_min < 0 THEN -ra_min ELSE 0 END) AS minuti_anticipo_tot,
            SUM(COALESCE(ra_min, 0)) AS minuti_netti_tot,
            AVG(ra_min) AS ritardo_medio,
            quantile_cont(ra_min, 0.5) AS ritardo_mediano,
            quantile_cont(ra_min, 0.9) AS p90,
            quantile_cont(ra_min, 0.95) AS p95
          FROM trains_clean
          GROUP BY 1,2
          ORDER BY 1,2
          """).fetchdf()

          kpi_mese_cat = con.execute("""
          SELECT
            mese,
            categoria,
            COUNT(*) AS corse_osservate,
            COUNT(ra_min) AS effettuate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS cancellate,
            0 AS soppresse,
            0 AS parzialmente_cancellate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS info_mancante,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min = 0 THEN 1 ELSE 0 END) AS in_orario,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min > 0 THEN 1 ELSE 0 END) AS in_ritardo,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min < 0 THEN 1 ELSE 0 END) AS in_anticipo,
            SUM(CASE WHEN ra_min > 5 THEN 1 ELSE 0 END) AS oltre_5,
            SUM(CASE WHEN ra_min > 10 THEN 1 ELSE 0 END) AS oltre_10,
            SUM(CASE WHEN ra_min > 15 THEN 1 ELSE 0 END) AS oltre_15,
            SUM(CASE WHEN ra_min > 30 THEN 1 ELSE 0 END) AS oltre_30,
            SUM(CASE WHEN ra_min > 60 THEN 1 ELSE 0 END) AS oltre_60,
            SUM(CASE WHEN ra_min > 0 THEN ra_min ELSE 0 END) AS minuti_ritardo_tot,
            SUM(CASE WHEN ra_min < 0 THEN -ra_min ELSE 0 END) AS minuti_anticipo_tot,
            SUM(COALESCE(ra_min, 0)) AS minuti_netti_tot,
            AVG(ra_min) AS ritardo_medio,
            quantile_cont(ra_min, 0.5) AS ritardo_mediano,
            quantile_cont(ra_min, 0.9) AS p90,
            quantile_cont(ra_min, 0.95) AS p95
          FROM trains_clean
          GROUP BY 1,2
          ORDER BY 1,2
          """).fetchdf()

          st_nodo = con.execute("""
          SELECT
            mese,
            categoria,
            stazione,
            COUNT(*) AS corse_osservate,
            COUNT(ra_min) AS effettuate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS cancellate,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min = 0 THEN 1 ELSE 0 END) AS in_orario,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min > 0 THEN 1 ELSE 0 END) AS in_ritardo,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min < 0 THEN 1 ELSE 0 END) AS in_anticipo,
            AVG(ra_min) AS ritardo_medio,
            quantile_cont(ra_min, 0.5) AS ritardo_mediano,
            quantile_cont(ra_min, 0.9) AS p90,
            quantile_cont(ra_min, 0.95) AS p95
          FROM stops_clean
          GROUP BY 1,2,3
          ORDER BY 1,2,3
          """).fetchdf()

          st_ruolo = con.execute("""
          SELECT
            mese,
            categoria,
            ruolo,
            stazione,
            COUNT(*) AS corse_osservate,
            COUNT(ra_min) AS effettuate,
            SUM(CASE WHEN ra_min IS NULL THEN 1 ELSE 0 END) AS cancellate,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min = 0 THEN 1 ELSE 0 END) AS in_orario,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min > 0 THEN 1 ELSE 0 END) AS in_ritardo,
            SUM(CASE WHEN ra_min IS NOT NULL AND ra_min < 0 THEN 1 ELSE 0 END) AS in_anticipo,
            AVG(ra_min) AS ritardo_medio,
            quantile_cont(ra_min, 0.5) AS ritardo_mediano,
            quantile_cont(ra_min, 0.9) AS p90,
            quantile_cont(ra_min, 0.95) AS p95
          FROM stops_clean
          GROUP BY 1,2,3,4
          ORDER BY 1,2,3,4
          """).fetchdf()

          od = con.execute("""
          SELECT
            mese,
            categoria,
            origine,
            destinazione,
            COUNT(*) AS corse_osservate,
            COUNT(ra_min) AS effettuate,
            AVG(ra_min) AS ritardo_medio,
            quantile_cont(ra_min, 0.9) AS p90
          FROM trains_clean
          GROUP BY 1,2,3,4
          ORDER BY 1,2,3,4
          """).fetchdf()

          con.close()

          merge_csv(DOCS / "kpi_giorno.csv", align_to_existing_schema(DOCS / "kpi_giorno.csv", kpi_giorno), ["giorno"])
          merge_csv(DOCS / "kpi_mese.csv", align_to_existing_schema(DOCS / "kpi_mese.csv", kpi_mese), ["mese"])
          merge_csv(DOCS / "kpi_giorno_categoria.csv", align_to_existing_schema(DOCS / "kpi_giorno_categoria.csv", kpi_giorno_cat), ["giorno","categoria"])
          merge_csv(DOCS / "kpi_mese_categoria.csv", align_to_existing_schema(DOCS / "kpi_mese_categoria.csv", kpi_mese_cat), ["mese","categoria"])

          merge_csv(DOCS / "stazioni_mese_categoria_nodo.csv", align_to_existing_schema(DOCS / "stazioni_mese_categoria_nodo.csv", st_nodo), ["mese","categoria","stazione"])
          merge_csv(DOCS / "stazioni_mese_categoria_ruolo.csv", align_to_existing_schema(DOCS / "stazioni_mese_categoria_ruolo.csv", st_ruolo), ["mese","categoria","ruolo","stazione"])

          if (DOCS / "od_mese_categoria.csv").exists():
              od2 = align_to_existing_schema(DOCS / "od_mese_categoria.csv", od)
              merge_csv(DOCS / "od_mese_categoria.csv", od2, ["mese","categoria","origine","destinazione"])

          if (DOCS / "hist_mese_categoria.csv").exists():
              trains_df = pd.read_parquet(trains_path)
              if "ref_day" in trains_df.columns and ("c" in trains_df.columns) and ("ra" in trains_df.columns):
                  d = trains_df[["ref_day","c","ra"]].copy()
                  d["mese"] = pd.to_datetime(d["ref_day"], format="%Y%m%d", errors="coerce").dt.to_period("M").dt.to_timestamp()
                  d["categoria"] = d["c"].astype(str).fillna("UNK")
                  def parse_ra(x):
                      try:
                          s = str(x).strip().lower()
                          if s in ("n","n.d.","nd",""):
                              return None
                          return float(s)
                      except Exception:
                          return None
                  d["ra_min"] = d["ra"].map(parse_ra)
                  bins = [-1e9, -60, -30, -15, -10, -5, 0, 5, 10, 15, 30, 60, 1e9]
                  labels = ["<-60","-60:-30","-30:-15","-15:-10","-10:-5","-5:0","0:5","5:10","10:15","15:30","30:60",">60"]
                  d = d.dropna(subset=["mese","categoria","ra_min"])
                  d["classe"] = pd.cut(d["ra_min"], bins=bins, labels=labels, right=False)
                  h = d.groupby(["mese","categoria","classe"], dropna=False).size().reset_index(name="count")
                  h = h.rename(columns={"classe":"bin"})
                  h["mese"] = pd.to_datetime(h["mese"]).dt.date
                  h = align_to_existing_schema(DOCS / "hist_mese_categoria.csv", h)
                  merge_csv(DOCS / "hist_mese_categoria.csv", h, ["mese","categoria","bin"])

          stations_file = DOCS / "stations_dim.csv"
          if stations_file.exists():
              st_names = pd.read_csv(stations_file)
              existing_colset = st_names.columns.tolist()
          else:
              st_names = pd.DataFrame(columns=["stazione","lat","lon"])
              existing_colset = st_names.columns.tolist()

          st_unique = pd.read_csv(DOCS / "stazioni_mese_categoria_nodo.csv")[["stazione"]].dropna().drop_duplicates()
          if "stazione" not in st_names.columns:
              st_names["stazione"] = pd.Series(dtype="object")
          missing = st_unique[~st_unique["stazione"].isin(st_names["stazione"].astype(str))]
          if len(missing):
              add = pd.DataFrame({"stazione": missing["stazione"].astype(str)})
              for c in st_names.columns:
                  if c not in add.columns:
                      add[c] = pd.NA
              add = add[st_names.columns]
              st_names = pd.concat([st_names, add], ignore_index=True)
              st_names = st_names.drop_duplicates(subset=["stazione"], keep="first")
              st_names.to_csv(stations_file, index=False)

          (ROOT / "docs" / ".nojekyll").touch()
          print("updated docs/data")
          PY

      - name: Commit and push published datasets
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add docs/data docs/.nojekyll
          git commit -m "Backfill 2024-06 (KPI + stations + dist + OD) merge" || echo "No changes"
          git push
